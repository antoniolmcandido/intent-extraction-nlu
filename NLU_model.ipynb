{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SKeTCvVfOgXu"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow_gpu==2.0.0-alpha0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OfBoYytHQGgs"
      },
      "outputs": [],
      "source": [
        "# !unzip src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oKH1Rn9IQggx"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'src')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nRvJ2KpDRKHw"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.keras'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32md:\\Projetos\\nlu-intent-extraction\\NLU_model.ipynb Célula 4\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m CuDNNLSTM\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m createVocabulary\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m loadVocabulary\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import CuDNNLSTM\n",
        "from utils import createVocabulary\n",
        "from utils import loadVocabulary\n",
        "from utils import computeF1Score\n",
        "from utils import DataProcessor\n",
        "\n",
        "parser = argparse.ArgumentParser(allow_abbrev=False)\n",
        "\n",
        "#Network\n",
        "parser.add_argument(\"--num_units\", type=int, default=128, help=\"Network size.\", dest='layer_size')\n",
        "parser.add_argument(\"--model_type\", type=str, default='full', help=\"\"\"full(default) | intent_only\n",
        "                                                                    full: full attention model\n",
        "                                                                    intent_only: intent attention model\"\"\")\n",
        "\n",
        "#Training Environment\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size.\")\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=20, help=\"Max epochs to train.\")\n",
        "parser.add_argument(\"--no_early_stop\", action='store_false',dest='early_stop', help=\"Disable early stop, which is based on sentence level accuracy.\")\n",
        "parser.add_argument(\"--patience\", type=int, default=5, help=\"Patience to wait before stop.\")\n",
        "\n",
        "#Model and Vocab\n",
        "parser.add_argument(\"--dataset\", type=str, default=None, help=\"\"\"Type 'atis' or 'snips' to use dataset provided by us or enter what ever you named your own dataset.\n",
        "                Note, if you don't want to use this part, enter --dataset=''. It can not be None\"\"\")\n",
        "parser.add_argument(\"--model_path\", type=str, default='./src/model', help=\"Path to save model.\")\n",
        "parser.add_argument(\"--vocab_path\", type=str, default='./src/vocab', help=\"Path to vocabulary files.\")\n",
        "\n",
        "#Data\n",
        "parser.add_argument(\"--train_data_path\", type=str, default='train', help=\"Path to training data files.\")\n",
        "parser.add_argument(\"--test_data_path\", type=str, default='test', help=\"Path to testing data files.\")\n",
        "parser.add_argument(\"--valid_data_path\", type=str, default='valid', help=\"Path to validation data files.\")\n",
        "parser.add_argument(\"--input_file\", type=str, default='seq.in', help=\"Input file name.\")\n",
        "parser.add_argument(\"--slot_file\", type=str, default='seq.out', help=\"Slot file name.\")\n",
        "parser.add_argument(\"--intent_file\", type=str, default='label', help=\"Intent file name.\")\n",
        "\n",
        "arg=parser.parse_args([\"--dataset\",\"snips\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oJw4ZYsyRpJn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_size = 64\n",
            "dataset = snips\n",
            "early_stop = True\n",
            "input_file = seq.in\n",
            "intent_file = label\n",
            "layer_size = 128\n",
            "max_epochs = 20\n",
            "model_path = ./src/model\n",
            "model_type = full\n",
            "patience = 5\n",
            "slot_file = seq.out\n",
            "test_data_path = test\n",
            "train_data_path = train\n",
            "valid_data_path = valid\n",
            "vocab_path = ./src/vocab\n",
            "\n",
            "use snips dataset\n"
          ]
        }
      ],
      "source": [
        "#Print arguments\n",
        "for k,v in sorted(vars(arg).items()):\n",
        "    print(k,'=',v)\n",
        "print()\n",
        "\n",
        "if arg.model_type == 'full':\n",
        "    add_final_state_to_intent = True\n",
        "    remove_slot_attn = False\n",
        "elif arg.model_type == 'intent_only':\n",
        "    add_final_state_to_intent = True\n",
        "    remove_slot_attn = True\n",
        "else:\n",
        "    print('unknown model type!')\n",
        "    exit(1)\n",
        "\n",
        "#full path to data will be: ./data + dataset + train/test/valid\n",
        "if arg.dataset == None:\n",
        "    print('name of dataset can not be None')\n",
        "    exit(1)\n",
        "elif arg.dataset == 'snips':\n",
        "    print('use snips dataset')\n",
        "elif arg.dataset == 'atis':\n",
        "    print('use atis dataset')\n",
        "else:\n",
        "    print('use own dataset: ',arg.dataset)\n",
        "full_train_path = os.path.join('./src/data',arg.dataset,arg.train_data_path)\n",
        "full_test_path = os.path.join('./src/data',arg.dataset,arg.test_data_path)\n",
        "full_valid_path = os.path.join('./src/data',arg.dataset,arg.valid_data_path)\n",
        "\n",
        "createVocabulary(os.path.join(full_train_path, arg.input_file), os.path.join(arg.vocab_path, 'in_vocab'))\n",
        "createVocabulary(os.path.join(full_train_path, arg.slot_file), os.path.join(arg.vocab_path, 'slot_vocab'))\n",
        "createVocabulary(os.path.join(full_train_path, arg.intent_file), os.path.join(arg.vocab_path, 'intent_vocab'))\n",
        "\n",
        "in_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'in_vocab'))\n",
        "slot_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'slot_vocab'))\n",
        "intent_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'intent_vocab'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XxCeThXUK1bu"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units, kernel_regularizer = tf.keras.regularizers.l2(0.02))\n",
        "        self.W2 = tf.keras.layers.Dense(units,kernel_regularizer = tf.keras.regularizers.l2(0.02))\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "\n",
        "        # score shape == (batch_size, max_length, hidden_size)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uj3aYLIBrpds"
      },
      "outputs": [],
      "source": [
        "#both query and values are time distributed\n",
        "class CustomBahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(CustomBahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units,kernel_regularizer = tf.keras.regularizers.l2(0.02))\n",
        "        self.W2 = tf.keras.layers.Conv1D(units,5,1,'same')\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        #hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        hidden_with_time_axis = query\n",
        "\n",
        "        # score shape == (batch_size, max_length, hidden_size)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        #context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LfpGhBsrrpp-"
      },
      "outputs": [],
      "source": [
        "class SlotGate(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(SlotGate, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units, kernel_regularizer = tf.keras.regularizers.l2(0.02))\n",
        "        self.W2 = tf.keras.layers.Dense(units, kernel_regularizer = tf.keras.regularizers.l2(0.02))\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, hidden_size)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        #context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "22Bbh5PQWFbc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "class CustomDropout(layers.Layer):\n",
        "  def __init__(self, rate, **kwargs):\n",
        "    super(CustomDropout, self).__init__(**kwargs)\n",
        "    self.rate = rate\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    if training:\n",
        "        return tf.nn.dropout(inputs, rate=self.rate)\n",
        "    return inputs\n",
        "  \n",
        "class BaselineModel(tf.keras.Model):\n",
        "  def __init__(self, input_size, slot_size, intent_size, layer_size = 128):\n",
        "    super(BaselineModel, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(input_size, layer_size)\n",
        "    self.bilstm = tf.keras.layers.Bidirectional(CuDNNLSTM(layer_size, return_sequences=True,return_state=True))\n",
        "    self.dropout = CustomDropout(0.5)\n",
        "    self.intent_out = tf.keras.layers.Dense(intent_size, activation=None)\n",
        "    self.slot_out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(slot_size, activation=None))\n",
        "  \n",
        "  @tf.function  \n",
        "  def call(self, inputs, sequence_length, isTraining=True):\n",
        "    x = self.embedding(inputs)\n",
        "    state_outputs, forward_h, forward_c, backward_h, backward_c = self.bilstm(x)\n",
        "    \n",
        "    state_outputs = self.dropout(state_outputs, isTraining)\n",
        "    forward_h = self.dropout(forward_h, isTraining)\n",
        "    backward_h = self.dropout(backward_h, isTraining)\n",
        "   \n",
        "    final_state = tf.keras.layers.concatenate([forward_h,backward_h])\n",
        "    intent = self.intent_out(final_state)\n",
        "    slots = self.slot_out(state_outputs)\n",
        "    outputs = [slots, intent]\n",
        "    return outputs    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YjRHVwM6s5xH"
      },
      "outputs": [],
      "source": [
        "class SlotGatedModel(tf.keras.Model):\n",
        "  def __init__(self, input_size, slot_size, intent_size, layer_size = 128):\n",
        "    super(SlotGatedModel, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(input_size, layer_size)\n",
        "    self.bilstm = tf.keras.layers.Bidirectional(CuDNNLSTM(layer_size, return_sequences=True,return_state=True))\n",
        "    self.dropout = CustomDropout(0.5)\n",
        "    \n",
        "    self.attn_size = 2*layer_size\n",
        "    self.slot_att = CustomBahdanauAttention(self.attn_size)\n",
        "    self.intent_att = BahdanauAttention(self.attn_size)\n",
        "    self.slot_gate = SlotGate(self.attn_size)\n",
        "    \n",
        "    self.intent_out = tf.keras.layers.Dense(intent_size, activation=None)\n",
        "    self.slot_out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(slot_size, activation=None))\n",
        "  \n",
        "  @tf.function  \n",
        "  def call(self, inputs, sequence_length, isTraining=True):\n",
        "    x = self.embedding(inputs)\n",
        "    state_outputs, forward_h, forward_c, backward_h, backward_c = self.bilstm(x)\n",
        "    \n",
        "    state_outputs = self.dropout(state_outputs, isTraining)\n",
        "    forward_h = self.dropout(forward_h, isTraining)\n",
        "    backward_h = self.dropout(backward_h, isTraining)\n",
        "    final_state = tf.keras.layers.concatenate([forward_h,backward_h])\n",
        "    \n",
        "    slot_d, _ = self.slot_att(state_outputs, state_outputs)\n",
        "    intent_d, _ = self.intent_att(final_state, state_outputs)\n",
        "    \n",
        "    intent_fd = tf.keras.layers.concatenate([intent_d,final_state], -1)\n",
        "    slot_gated,_ = self.slot_gate(intent_fd, slot_d)\n",
        "    slot_fd = tf.keras.layers.concatenate([slot_gated,state_outputs], -1)\n",
        "    \n",
        "    \n",
        "    intent = self.intent_out(intent_fd)\n",
        "    slots = self.slot_out(slot_fd)\n",
        "    outputs = [slots, intent]\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "d4WTGWQX4AH7"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = SlotGatedModel(len(in_vocab['vocab']), len(slot_vocab['vocab']), len(intent_vocab['vocab']), layer_size=arg.layer_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "geNgBiAPXffN"
      },
      "outputs": [],
      "source": [
        "def valid(in_path, slot_path, intent_path):\n",
        "    data_processor_valid = DataProcessor(in_path, slot_path, intent_path, in_vocab, slot_vocab, intent_vocab)\n",
        "    pred_intents = []\n",
        "    correct_intents = []\n",
        "    slot_outputs = []\n",
        "    correct_slots = []\n",
        "    input_words = []\n",
        "\n",
        "    #used to gate\n",
        "    #gate_seq = []\n",
        "    while True:\n",
        "        in_data, slot_data, slot_weight, length, intents, in_seq, slot_seq, intent_seq = data_processor_valid.get_batch(arg.batch_size)\n",
        "        #feed_dict = {input_data.name: in_data, sequence_length.name: length}\n",
        "        #ret = sess.run(inference_outputs, feed_dict)\n",
        "        slots, intent = model(in_data, length, isTraining = False)\n",
        "        for i in np.array(intent):\n",
        "            pred_intents.append(np.argmax(i))\n",
        "        for i in intents:\n",
        "            correct_intents.append(i)\n",
        "\n",
        "        pred_slots = slots\n",
        "        for p, t, i, l in zip(pred_slots, slot_data, in_data, length):\n",
        "            p = np.argmax(p, 1)\n",
        "            tmp_pred = []\n",
        "            tmp_correct = []\n",
        "            tmp_input = []\n",
        "            for j in range(l):\n",
        "                tmp_pred.append(slot_vocab['rev'][p[j]])\n",
        "                tmp_correct.append(slot_vocab['rev'][t[j]])\n",
        "                tmp_input.append(in_vocab['rev'][i[j]])\n",
        "\n",
        "            slot_outputs.append(tmp_pred)\n",
        "            correct_slots.append(tmp_correct)\n",
        "            input_words.append(tmp_input)\n",
        "\n",
        "        if data_processor_valid.end == 1:\n",
        "            break\n",
        "\n",
        "    pred_intents = np.array(pred_intents)\n",
        "    correct_intents = np.array(correct_intents)\n",
        "    accuracy = (pred_intents==correct_intents)\n",
        "    semantic_error = accuracy\n",
        "    accuracy = accuracy.astype(float)\n",
        "    accuracy = np.mean(accuracy)*100.0\n",
        "\n",
        "    index = 0\n",
        "    for t, p in zip(correct_slots, slot_outputs):\n",
        "        # Process Semantic Error\n",
        "        if len(t) != len(p):\n",
        "            raise ValueError('Error!!')\n",
        "\n",
        "        for j in range(len(t)):\n",
        "            if p[j] != t[j]:\n",
        "                semantic_error[index] = False\n",
        "                break\n",
        "        index += 1\n",
        "    semantic_error = semantic_error.astype(float)\n",
        "    semantic_error = np.mean(semantic_error)*100.0\n",
        "\n",
        "    f1, precision, recall = computeF1Score(correct_slots, slot_outputs)\n",
        "    print('slot f1: ' + str(f1) + '\\tintent accuracy: ' + str(accuracy) + '\\tsemantic_error: ' + str(semantic_error))\n",
        "    #print('intent accuracy: ' + str(accuracy))\n",
        "    #print('semantic error(intent, slots are all correct): ' + str(semantic_error))\n",
        "\n",
        "    data_processor_valid.close()\n",
        "    return f1,accuracy,semantic_error,pred_intents,correct_intents,slot_outputs,correct_slots,input_words #,gate_seq  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rrfg5-eOY9Do"
      },
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer \"slot_gated_model\" \"                 f\"(type SlotGatedModel).\n\nNo OpKernel was registered to support Op 'CudnnRNNV2' used by {{node bidirectional/forward_cu_dnnlstm/CudnnRNNV2}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true]\nRegistered devices: [CPU, GPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[bidirectional/forward_cu_dnnlstm/CudnnRNNV2]] [Op:__forward_call_5104]\n\nCall arguments received by layer \"slot_gated_model\" \"                 f\"(type SlotGatedModel):\n  • inputs=tf.Tensor(shape=(64, 50), dtype=int32)\n  • sequence_length=array([ 8,  9, 16,  6, 10, 10,  7,  7,  3,  7, 15,  5, 12,  9, 15,  9, 15,\n        9,  9, 10,  9,  9,  9,  8,  7,  8,  4,  5,  9,  3,  8,  9,  6,  7,\n        9, 11, 12,  6,  9,  7,  5, 11, 10,  6, 15, 11,  6,  5,  6, 12,  5,\n        8,  8, 11, 15,  8,  8, 12,  9,  9,  4,  7, 14,  8])\n  • isTraining=True",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32md:\\Projetos\\nlu-intent-extraction\\NLU_model.ipynb Célula 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m in_data, slot_labels, slot_weights, length, intent_labels,in_seq,_,_ \u001b[39m=\u001b[39m data_processor\u001b[39m.\u001b[39mget_batch(arg\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m   slots, intent \u001b[39m=\u001b[39m model(in_data, length, isTraining \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   intent_loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msparse_softmax_cross_entropy_with_logits(labels\u001b[39m=\u001b[39mintent_labels, logits\u001b[39m=\u001b[39mintent)  \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   \u001b[39m#slot_loss\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"slot_gated_model\" \"                 f\"(type SlotGatedModel).\n\nNo OpKernel was registered to support Op 'CudnnRNNV2' used by {{node bidirectional/forward_cu_dnnlstm/CudnnRNNV2}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true]\nRegistered devices: [CPU, GPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[bidirectional/forward_cu_dnnlstm/CudnnRNNV2]] [Op:__forward_call_5104]\n\nCall arguments received by layer \"slot_gated_model\" \"                 f\"(type SlotGatedModel):\n  • inputs=tf.Tensor(shape=(64, 50), dtype=int32)\n  • sequence_length=array([ 8,  9, 16,  6, 10, 10,  7,  7,  3,  7, 15,  5, 12,  9, 15,  9, 15,\n        9,  9, 10,  9,  9,  9,  8,  7,  8,  4,  5,  9,  3,  8,  9,  6,  7,\n        9, 11, 12,  6,  9,  7,  5, 11, 10,  6, 15, 11,  6,  5,  6, 12,  5,\n        8,  8, 11, 15,  8,  8, 12,  9,  9,  4,  7, 14,  8])\n  • isTraining=True"
          ]
        }
      ],
      "source": [
        "#training\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(-1), optimizer=opt, net=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, arg.model_path + \"/\" + arg.dataset + '/tf_ckpts', max_to_keep=3)\n",
        "data_processor = None\n",
        "valid_err = 0\n",
        "no_improve= 0\n",
        "save_path = os.path.join(arg.model_path , str(arg.dataset) + \"/\")\n",
        "for epoch in range(50):\n",
        "    while True:\n",
        "        if data_processor == None:\n",
        "            i_loss = 0\n",
        "            s_loss = 0\n",
        "            batches = 0\n",
        "            data_processor = DataProcessor(os.path.join(full_train_path, arg.input_file), os.path.join(full_train_path, arg.slot_file), os.path.join(full_train_path, arg.intent_file), in_vocab, slot_vocab, intent_vocab)\n",
        "        in_data, slot_labels, slot_weights, length, intent_labels,in_seq,_,_ = data_processor.get_batch(arg.batch_size)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "          slots, intent = model(in_data, length, isTraining = True)\n",
        "          intent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=intent_labels, logits=intent)  \n",
        "          #slot_loss\n",
        "          slots_out = tf.reshape(slots, [-1,len(slot_vocab['vocab'])])\n",
        "          slots_shape = tf.shape(slot_labels)\n",
        "          slot_reshape = tf.reshape(slot_labels, [-1])\n",
        "          crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=slot_reshape, logits=slots_out)\n",
        "          crossent = tf.reshape(crossent, slots_shape)\n",
        "          slot_loss = tf.reduce_sum(crossent*slot_weights, 1)\n",
        "          total_size = tf.reduce_sum(slot_weights, 1)\n",
        "          total_size += 1e-12\n",
        "          slot_loss = slot_loss / total_size\n",
        "          \n",
        "          total_loss = intent_loss + slot_loss + tf.reduce_sum(model.losses)\n",
        "        \n",
        "        grads = tape.gradient(total_loss, model.trainable_weights)\n",
        "        opt.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        s_loss = s_loss + tf.reduce_sum(slot_loss)/tf.cast(arg.batch_size, tf.float32)\n",
        "        i_loss = i_loss +  tf.reduce_sum(intent_loss)/tf.cast(arg.batch_size, tf.float32)\n",
        "        batches = batches + 1\n",
        "        if data_processor.end == 1:\n",
        "            data_processor.close()\n",
        "            data_processor = None\n",
        "            break\n",
        "            \n",
        "    #print(\"Training Epoch: \" ,epoch,\" Slot Loss: \",s_loss/batches, \" Intent_Loss: \", i_loss/batches)\n",
        "    print(\"EPOCH: \", epoch, \" *******************************************************************\")\n",
        "    print('Train:', end=\"\\t\")\n",
        "    _ = valid(os.path.join(full_train_path, arg.input_file), os.path.join(full_train_path, arg.slot_file), os.path.join(full_train_path, arg.intent_file))\n",
        "    \n",
        "    print('Valid:', end=\"\\t\")\n",
        "    epoch_valid_slot, epoch_valid_intent, epoch_valid_err,valid_pred_intent,valid_correct_intent,valid_pred_slot,valid_correct_slot,valid_words = valid(os.path.join(full_valid_path, arg.input_file), os.path.join(full_valid_path, arg.slot_file), os.path.join(full_valid_path, arg.intent_file))\n",
        "\n",
        "    print('Test:', end=\"\\t\")\n",
        "    epoch_test_slot, epoch_test_intent, epoch_test_err,test_pred_intent,test_correct_intent,test_pred_slot,test_correct_slot,test_words = valid(os.path.join(full_test_path, arg.input_file), os.path.join(full_test_path, arg.slot_file), os.path.join(full_test_path, arg.intent_file))\n",
        "    \n",
        "    ckpt.step.assign_add(1)\n",
        "    if epoch_valid_err <= valid_err:\n",
        "        no_improve += 1\n",
        "    else:\n",
        "        valid_err = epoch_valid_err\n",
        "        no_improve = 0\n",
        "        print(\"Saving\", str(ckpt.step), \"with valid accuracy:\", valid_err   )\n",
        "        save_path = manager.save()\n",
        "\n",
        "    if arg.early_stop == True:\n",
        "        if no_improve > arg.patience:\n",
        "            print(\"EARLY BREAK\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "u-yr9QkEvCK6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"slot_gated_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  1439104   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  multiple                 264192    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " custom_dropout (CustomDropo  multiple                 0         \n",
            " ut)                                                             \n",
            "                                                                 \n",
            " custom_bahdanau_attention (  multiple                 393985    \n",
            " CustomBahdanauAttention)                                        \n",
            "                                                                 \n",
            " bahdanau_attention (Bahdana  multiple                 131841    \n",
            " uAttention)                                                     \n",
            "                                                                 \n",
            " slot_gate (SlotGate)        multiple                  197377    \n",
            "                                                                 \n",
            " dense_8 (Dense)             multiple                  4617      \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  multiple                 37962     \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,469,078\n",
            "Trainable params: 2,469,078\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4o15eK9npnOS"
      },
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer \"baseline_model\" \"                 f\"(type BaselineModel).\n\nNo OpKernel was registered to support Op 'CudnnRNNV2' used by {{node bidirectional/forward_cu_dnnlstm/CudnnRNNV2}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true]\nRegistered devices: [CPU, GPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[bidirectional/forward_cu_dnnlstm/CudnnRNNV2]] [Op:__inference_call_5897]\n\nCall arguments received by layer \"baseline_model\" \"                 f\"(type BaselineModel):\n  • inputs=tf.Tensor(shape=(700, 50), dtype=int32)\n  • sequence_length=array([ 8, 18, 13,  9,  8, 12, 12,  7,  9, 13,  7,  7,  4, 15, 10, 12,  6,\n        7, 11,  9,  8, 11, 10,  4,  5, 10,  7, 14,  9,  5,  7,  8,  4,  5,\n       10,  8,  9,  4, 12,  5, 10, 15,  6,  8,  6, 20,  5,  8, 10,  8,  7,\n       16,  9,  9, 14, 12,  6,  7,  6,  6, 12,  5, 11,  8,  9,  7, 12, 10,\n        6, 12,  5,  6,  9,  9,  9,  7,  8,  5,  9, 11,  7, 10,  9,  9,  7,\n        6,  8, 12,  8,  4,  7,  6,  6,  8, 14, 11,  6,  8,  9,  6,  7,  7,\n        7, 10,  7, 17, 10,  4,  5,  7,  4, 13, 11,  9, 11,  5,  9,  8, 11,\n        7,  6, 13, 12, 17, 13,  7, 13,  6, 15, 11,  8,  4,  9,  6,  6,  6,\n        9,  6,  9,  3, 14, 15,  7,  7,  7,  8, 10,  7,  7, 14,  8, 10,  7,\n        5, 11, 11,  8, 11,  7, 17,  9,  9,  5, 17, 13, 11, 16,  6,  9,  6,\n        8, 13, 11,  7,  8,  7,  9,  5, 14,  6,  4,  6, 11,  8, 13,  5, 11,\n        7, 19, 12,  6,  6, 15, 11, 14,  7,  5, 15, 11,  5, 10, 12, 11, 20,\n        7,  8,  7,  4, 14, 13,  7,  6,  7,  9,  9, 10,  5, 11,  7,  7,  4,\n        7, 11, 10, 13, 10,  8, 14,  8, 13,  7,  6, 14, 14,  9,  7, 11, 10,\n       10,  6,  5, 15,  5,  9, 13,  8,  8,  4,  8,  5, 10,  7, 14,  7,  6,\n       16,  8, 12, 15,  7,  8, 13,  8, 13, 18, 13,  9,  6,  4,  9,  4, 10,\n       22,  9, 20,  6,  9, 12, 15, 14,  6, 12,  7,  9,  7,  9,  6, 11,  6,\n       12,  9, 10,  7,  8,  9, 12,  3,  9,  8,  9, 15,  3,  6, 10,  8,  3,\n        9,  8,  9,  6,  8, 19,  8,  8,  9, 12, 12,  3,  7,  3, 12,  9, 16,\n        5,  7,  8,  8, 16,  6, 11,  3,  7,  8,  9,  5,  7,  8,  8,  8, 14,\n        8,  8,  8,  7, 11, 10, 16, 11, 11, 16,  5,  6,  6,  5,  6, 10,  6,\n        5, 10,  7,  9, 12,  9, 11, 17,  6,  8, 12, 21,  8, 13,  8,  7,  8,\n       14, 15, 12,  6, 14,  6,  7,  5, 17, 18, 11,  9, 11, 16,  8, 15, 11,\n       14, 11, 10,  7,  3,  3, 13,  9, 10,  5, 11,  9,  7, 11, 14, 10,  8,\n        9,  8,  9, 10,  8,  9,  8, 10,  6,  7,  9, 10,  9,  9,  5, 13, 11,\n        6,  7, 10, 10,  9, 12, 10,  8, 12,  9, 10, 11, 13, 11,  6,  5, 13,\n        9, 12, 11,  7, 16, 13, 10,  8, 11, 16, 11,  8, 11, 10, 13,  7,  8,\n       10, 12, 13, 10,  5, 13,  5,  8,  8,  5,  6,  7,  9, 12, 13,  9,  7,\n       11,  7,  7,  6,  6,  6, 11, 14,  5,  8,  5, 10,  6,  6, 10, 10,  8,\n        7,  8,  7, 12, 10, 10, 10,  8,  8, 11, 12,  5, 16,  6,  7,  6, 10,\n        7,  5, 11,  7,  8, 10,  6,  9,  6, 14,  9,  6, 10,  6, 14,  5,  9,\n       11,  7,  9,  8, 14, 11, 13,  7, 11, 11, 11,  7, 14,  6,  8, 15,  9,\n        9,  7,  7,  6,  6,  5, 12,  8,  8,  7,  7,  8,  6,  6,  6,  7,  6,\n       12, 10, 12, 12,  9,  5, 11,  9, 11,  6,  9,  7,  4, 16,  8, 13, 11,\n        4, 12, 10, 10,  8, 24, 11,  7,  6, 16,  7,  6, 15,  6, 11,  8,  4,\n       12, 11,  8, 12, 11, 10,  9, 10, 10, 11,  6,  7, 11,  6,  8, 13, 10,\n        9,  6,  5,  5,  6,  4,  5,  7, 11,  7,  7, 12,  5,  5,  7,  7,  8,\n        4, 11,  5,  8,  4, 10,  9, 14,  6,  4,  5,  5, 12, 11, 11,  6,  7,\n        9,  8,  8, 16,  7, 13,  9,  9,  7, 10,  7,  4,  7, 11,  6, 17, 10,\n        8,  7,  7,  7, 11, 16, 13, 10,  9, 11, 10, 15,  8,  6,  8,  4,  9,\n        4,  7,  9,  8,  8, 13,  8,  9,  4, 13, 16, 13, 12,  4,  9,  7,  5,\n        8, 10,  7])\n  • isTraining=False",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32md:\\Projetos\\nlu-intent-extraction\\NLU_model.ipynb Célula 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m data_processor_test\u001b[39m.\u001b[39mclose()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m slots, intent \u001b[39m=\u001b[39m model(in_data, length, isTraining \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m t\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projetos/nlu-intent-extraction/NLU_model.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMilli seconds per query:\u001b[39m\u001b[39m\"\u001b[39m, (elapsed\u001b[39m*\u001b[39m\u001b[39m1000\u001b[39m)\u001b[39m/\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m100.0\u001b[39m))\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"baseline_model\" \"                 f\"(type BaselineModel).\n\nNo OpKernel was registered to support Op 'CudnnRNNV2' used by {{node bidirectional/forward_cu_dnnlstm/CudnnRNNV2}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true]\nRegistered devices: [CPU, GPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[bidirectional/forward_cu_dnnlstm/CudnnRNNV2]] [Op:__inference_call_5897]\n\nCall arguments received by layer \"baseline_model\" \"                 f\"(type BaselineModel):\n  • inputs=tf.Tensor(shape=(700, 50), dtype=int32)\n  • sequence_length=array([ 8, 18, 13,  9,  8, 12, 12,  7,  9, 13,  7,  7,  4, 15, 10, 12,  6,\n        7, 11,  9,  8, 11, 10,  4,  5, 10,  7, 14,  9,  5,  7,  8,  4,  5,\n       10,  8,  9,  4, 12,  5, 10, 15,  6,  8,  6, 20,  5,  8, 10,  8,  7,\n       16,  9,  9, 14, 12,  6,  7,  6,  6, 12,  5, 11,  8,  9,  7, 12, 10,\n        6, 12,  5,  6,  9,  9,  9,  7,  8,  5,  9, 11,  7, 10,  9,  9,  7,\n        6,  8, 12,  8,  4,  7,  6,  6,  8, 14, 11,  6,  8,  9,  6,  7,  7,\n        7, 10,  7, 17, 10,  4,  5,  7,  4, 13, 11,  9, 11,  5,  9,  8, 11,\n        7,  6, 13, 12, 17, 13,  7, 13,  6, 15, 11,  8,  4,  9,  6,  6,  6,\n        9,  6,  9,  3, 14, 15,  7,  7,  7,  8, 10,  7,  7, 14,  8, 10,  7,\n        5, 11, 11,  8, 11,  7, 17,  9,  9,  5, 17, 13, 11, 16,  6,  9,  6,\n        8, 13, 11,  7,  8,  7,  9,  5, 14,  6,  4,  6, 11,  8, 13,  5, 11,\n        7, 19, 12,  6,  6, 15, 11, 14,  7,  5, 15, 11,  5, 10, 12, 11, 20,\n        7,  8,  7,  4, 14, 13,  7,  6,  7,  9,  9, 10,  5, 11,  7,  7,  4,\n        7, 11, 10, 13, 10,  8, 14,  8, 13,  7,  6, 14, 14,  9,  7, 11, 10,\n       10,  6,  5, 15,  5,  9, 13,  8,  8,  4,  8,  5, 10,  7, 14,  7,  6,\n       16,  8, 12, 15,  7,  8, 13,  8, 13, 18, 13,  9,  6,  4,  9,  4, 10,\n       22,  9, 20,  6,  9, 12, 15, 14,  6, 12,  7,  9,  7,  9,  6, 11,  6,\n       12,  9, 10,  7,  8,  9, 12,  3,  9,  8,  9, 15,  3,  6, 10,  8,  3,\n        9,  8,  9,  6,  8, 19,  8,  8,  9, 12, 12,  3,  7,  3, 12,  9, 16,\n        5,  7,  8,  8, 16,  6, 11,  3,  7,  8,  9,  5,  7,  8,  8,  8, 14,\n        8,  8,  8,  7, 11, 10, 16, 11, 11, 16,  5,  6,  6,  5,  6, 10,  6,\n        5, 10,  7,  9, 12,  9, 11, 17,  6,  8, 12, 21,  8, 13,  8,  7,  8,\n       14, 15, 12,  6, 14,  6,  7,  5, 17, 18, 11,  9, 11, 16,  8, 15, 11,\n       14, 11, 10,  7,  3,  3, 13,  9, 10,  5, 11,  9,  7, 11, 14, 10,  8,\n        9,  8,  9, 10,  8,  9,  8, 10,  6,  7,  9, 10,  9,  9,  5, 13, 11,\n        6,  7, 10, 10,  9, 12, 10,  8, 12,  9, 10, 11, 13, 11,  6,  5, 13,\n        9, 12, 11,  7, 16, 13, 10,  8, 11, 16, 11,  8, 11, 10, 13,  7,  8,\n       10, 12, 13, 10,  5, 13,  5,  8,  8,  5,  6,  7,  9, 12, 13,  9,  7,\n       11,  7,  7,  6,  6,  6, 11, 14,  5,  8,  5, 10,  6,  6, 10, 10,  8,\n        7,  8,  7, 12, 10, 10, 10,  8,  8, 11, 12,  5, 16,  6,  7,  6, 10,\n        7,  5, 11,  7,  8, 10,  6,  9,  6, 14,  9,  6, 10,  6, 14,  5,  9,\n       11,  7,  9,  8, 14, 11, 13,  7, 11, 11, 11,  7, 14,  6,  8, 15,  9,\n        9,  7,  7,  6,  6,  5, 12,  8,  8,  7,  7,  8,  6,  6,  6,  7,  6,\n       12, 10, 12, 12,  9,  5, 11,  9, 11,  6,  9,  7,  4, 16,  8, 13, 11,\n        4, 12, 10, 10,  8, 24, 11,  7,  6, 16,  7,  6, 15,  6, 11,  8,  4,\n       12, 11,  8, 12, 11, 10,  9, 10, 10, 11,  6,  7, 11,  6,  8, 13, 10,\n        9,  6,  5,  5,  6,  4,  5,  7, 11,  7,  7, 12,  5,  5,  7,  7,  8,\n        4, 11,  5,  8,  4, 10,  9, 14,  6,  4,  5,  5, 12, 11, 11,  6,  7,\n        9,  8,  8, 16,  7, 13,  9,  9,  7, 10,  7,  4,  7, 11,  6, 17, 10,\n        8,  7,  7,  7, 11, 16, 13, 10,  9, 11, 10, 15,  8,  6,  8,  4,  9,\n        4,  7,  9,  8,  8, 13,  8,  9,  4, 13, 16, 13, 12,  4,  9,  7,  5,\n        8, 10,  7])\n  • isTraining=False"
          ]
        }
      ],
      "source": [
        "#Let's try to clear slate and reload maodel  .....\n",
        "import time\n",
        "tf.keras.backend.clear_session()\n",
        "if arg.dataset == 'atis':\n",
        "    test_batch = 893\n",
        "elif arg.dataset == 'snips':\n",
        "    test_batch = 700\n",
        "\n",
        "model = BaselineModel(len(in_vocab['vocab']), len(slot_vocab['vocab']), len(intent_vocab['vocab']), layer_size=arg.layer_size)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(-1), optimizer=opt, net=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, arg.model_path + \"/\" + arg.dataset + '/tf_ckpts', max_to_keep=3)\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "\n",
        "data_processor_test = DataProcessor(os.path.join(full_test_path, arg.input_file), os.path.join(full_test_path, arg.slot_file), os.path.join(full_test_path, arg.intent_file), in_vocab, slot_vocab, intent_vocab)\n",
        "in_data, slot_labels, slot_weights, length, intent_labels,in_seq,_,_ = data_processor_test.get_batch(test_batch)\n",
        "data_processor_test.close()\n",
        "\n",
        "t = time.perf_counter()\n",
        "slots, intent = model(in_data, length, isTraining = False)\n",
        "elapsed = time.perf_counter() - t\n",
        "print(\"Milli seconds per query:\", (elapsed*1000)/float(100.0))\n",
        "\n",
        "pred_intents = []\n",
        "correct_intents = []\n",
        "slot_outputs = []\n",
        "correct_slots = []\n",
        "input_words = []\n",
        "\n",
        "for i in np.array(intent):\n",
        "    pred_intents.append(np.argmax(i))\n",
        "for i in intent_labels:\n",
        "    correct_intents.append(i)\n",
        "\n",
        "pred_slots = slots\n",
        "for p, t, i, l in zip(pred_slots, slot_labels, in_data, length):\n",
        "    p = np.argmax(p, 1)\n",
        "    tmp_pred = []\n",
        "    tmp_correct = []\n",
        "    tmp_input = []\n",
        "    for j in range(l):\n",
        "        tmp_pred.append(slot_vocab['rev'][p[j]])\n",
        "        tmp_correct.append(slot_vocab['rev'][t[j]])\n",
        "        tmp_input.append(in_vocab['rev'][i[j]])\n",
        "\n",
        "    slot_outputs.append(tmp_pred)\n",
        "    correct_slots.append(tmp_correct)\n",
        "    input_words.append(tmp_input)\n",
        "\n",
        "pred_intents = np.array(pred_intents)\n",
        "correct_intents = np.array(correct_intents)\n",
        "accuracy = (pred_intents==correct_intents)\n",
        "semantic_error = accuracy\n",
        "accuracy = accuracy.astype(float)\n",
        "accuracy = np.mean(accuracy)*100.0\n",
        "\n",
        "index = 0\n",
        "for t, p in zip(correct_slots, slot_outputs):\n",
        "    # Process Semantic Error\n",
        "    if len(t) != len(p):\n",
        "        raise ValueError('Error!!')\n",
        "\n",
        "    for j in range(len(t)):\n",
        "        if p[j] != t[j]:\n",
        "            semantic_error[index] = False\n",
        "            break\n",
        "    index += 1\n",
        "semantic_error = semantic_error.astype(float)\n",
        "semantic_error = np.mean(semantic_error)*100.0\n",
        "\n",
        "f1, precision, recall = computeF1Score(correct_slots, slot_outputs)\n",
        "print('slot f1: ' + str(f1) + '\\tintent accuracy: ' + str(accuracy) + '\\tsemantic_accuracy: ' + str(semantic_error))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NLU.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
